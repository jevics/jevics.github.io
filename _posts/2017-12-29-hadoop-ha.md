---
layout: post
categories: Bigdata
title: Hadoop HDFS 文件系统HA高可用
date: 2017-01-23 14:23:02 +0800
description: Hadoop HDFS 文件系统HA高可用
keywords: hadoop hdfs
---

![Hadoop](http://hadoop.apache.org/images/hadoop-logo.jpg)

### 概述
* Hadoop是一个由Apache基金会所开发的分布式系统基础架构。
用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力进行高速运算和存储。
* Hadoop实现了一个分布式文件系统（Hadoop Distributed File System），简称HDFS。HDFS有高容错性的特点，并且设计用来部署在低廉的（low-cost）硬件上；而且它提供高吞吐量（high throughput）来访问应用程序的数据，适合那些有着超大数据集（large data set）的应用程序。
* HDFS放宽了（relax）POSIX的要求，可以以流的形式访问（streaming access）文件系统中的数据。
* Hadoop的框架最核心的设计就是：HDFS和MapReduce。
* HDFS为海量的数据提供了存储，则MapReduce为海量的数据提供了计算。

>Hadoop2.x之后的版本提供了解决单点问题的方案－HA(HighAvailable 高可用) 执行步骤如下：
 

###  下载链接：
- [zookeeper下载地址](http://zookeeper.apache.org/releases.html)
- [hadoop2.x下载地址](http://apache.fayea.com/hadoop/common/)
- [JDK下载地址](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)

### 系统环境：CentOS 7.2
#### 创建Hadoop用户

``` sh
[root@ ~]# useradd hadoop && passwd hadoop
[root@ ~]# vim /etc/sudoers 
hadoop ALL=(root)    NOPASSWD:ALL 
[root@ ~]# mkdir -p /opt/source/hadoop && chown hadoop:hadoop /opt/source/hadoop -R
[root@ ~]# ls /opt/source/hadoop
drwxrwxr-x 10 hadoop hadoop 4096 Dec 22 13:25 hadoop
drwxr-xr-x 10 hadoop hadoop   4096 Dec 21 18:20 zookeeper
```

#### *安装JDK*
``` sh
[root@ ~]# ls /opt/source
drwxr-xr-x  8 root   root   4096 Sep 23 07:27 jdk1.8.0_111
[root@ ~]# ln -s /opt/source/jdk1.8.0_111 /opt/jdk
```

#### *配置hosts*

- 集群中所有机器的 `hosts` 配置要相同:

``` sh
10.211.55.12    nna　　# NameNode Active
10.211.55.13    nns　　# NameNode Standby
10.211.55.14    dn1　　# DataNode1
10.211.55.15    dn2　　# DataNode2
10.211.55.16    dn3　　# DataNode3
```

然后用 *scp* 命令，分发到各个节点,
这里以NNS节点为例子

``` sh
scp /etc/hosts hadoop@nns:/etc/
```
#### *SSH*
``` sh
输入如下命令：
$ ssh-keygen –t rsa
然后一路按回车键，最后在将id_rsa.pub写到authorized_keys，命令如下：
$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
在hadoop用户下，需要给authorized_keys赋予600的权限，不然免密码登陆无效。
在其他节点只需要使用 ssh-keygen –t rsa 命令，生产对应的公钥，然后将各个节点的id_rsa.pub追加到nna节点的authorized_keys中。
最后，将nna节点下的authorized_keys文件通过scp命令，分发到各个节点的 ~/.ssh/ 目录下。目录如下：
# 这里以NNS节点为例子
$ scp ~/.ssh/authorized_keys hadoop@nns:~/.ssh/
然后使用ssh命令相互登录，看是否实现了免密码登录，登录命令如下：
# 这里以nns节点为例子
$ ssh nns
若登录过程中木有提示需要输入密码，即表示密码配置成功。

关闭防火墙
[root@ ~]# systemctl stop firewalld && systemctl disable firewalld
```

#### ZooKeeper（安装-启动-验证）

``` sh
安装
将下载好的安装包，解压到指定位置，这里为直接解压到当前位置，命令如下：
[hadoop@ ~] tar -zxvf zk-{version}.tar.gz
修改zk配置，将zk安装目录下conf/zoo_sample.cfg重命名zoo.cfg，修改其中的内容：
[hadoop@es1 ~]$ vim zoo.cfg
# The number of milliseconds of each tick
# 服务器与客户端之间交互的基本时间单元（ms） 
tickTime=2000   

# The number of ticks that the initial  
# synchronization phase can take 
# zookeeper所能接受的客户端数量 
initLimit=10  

# The number of ticks that can pass between  
# sending a request and getting an acknowledgement 
# 服务器和客户端之间请求和应答之间的时间间隔 
syncLimit=5

# the directory where the snapshot is stored. 
# do not use /tmp for storage, /tmp here is just  
# example sakes. 
# 保存zookeeper数据，日志的路径
dataDir=/home/hadoop/data/zookeeper

# the port at which the clients will connect 
# 客户端与zookeeper相互交互的端口 
clientPort=2181 
server.1= dn1:2888:3888 
server.2= dn2:2888:3888 
server.3= dn3:2888:3888

#server.A=B:C:D  
其中A是一个数字，代表这是第几号服务器；
B是服务器的IP地址；
C表示服务器与群集中的“领导者”交换信息的端口；当领导者失效后
D表示用来执行选举时服务器相互通信的端口。
接下来，在配置的dataDir目录下创建一个myid文件，里面写入一个0-255之间的一个随意数字，
每个zk上这个文件的数字要是不一样的，这些数字应该是从1开始
依次写每个服务器。
文件中序号要与dn节点下的zk配置序号一直，
如：server.1=dn1:2888:3888，那么dn1节点下的myid配置文件应该写上1。

>启动
分别在各个dn节点启动zk进程，命令如下：
[hadoop@es1 ~]$ bin/zkServer.sh start
然后，在各个节点输入jps命令，会出现如下进程：
[hadoop@es1 ~]$ jps
QuorumPeerMain

>验证
上面说的输入jps命令，若显示对应的进程，即表示启动成功，
同样我们也可以输入zk的状态命令查看，命令如下：
[hadoop@es1 ~]$ bin/zkServer.sh status
会出现一个leader和两个follower。

```

#### HDFS＋HA的结构图
##### HDFS配置HA的结构图如下所示：
![image](http://images.cnitblog.com/blog/666745/201502/251458129553368.png)

#### 上图大致架构包括：

1. 利用共享存储来在两个NN间同步edits信息。以前的HDFS是share nothing but NN，现在NN又share storage，这样其实是转移了单点故障的位置，但中高端的存储设备内部都有各种RAID以及冗余硬件，包括电源以及网卡等，比服务器的可靠性还是略有提高。通过NN内部每次元数据变动后的flush操作，加上NFS的close-to-open，数据的一致性得到了保证。

2. DN同时向两个NN汇报块信息。这是让Standby NN保持集群的最新状态的必须步骤。

3. 用于监视和控制NN进程的FailoverController进程。显然，我们不能在NN进程内部进行心跳等信息同步，最简单的原因，一次FullGC就可以让NN挂起十几分钟，所以，必须要有一个独立的短小精悍的watchdog来专门负责监控。这也是一个松耦合的设计，便于扩展或更改，目前版本里是用ZooKeeper（简称ZK）来做同步锁，但用户可以方便的把这个Zookeeper FailoverController（简称ZKFC）替换为其他的HA方案或leader选举方案。

4. 隔离（Fencing），防止脑裂，就是保证在任何时候只有一个主NN，包括三个方面：
>共享存储fencing，确保只有一个NN可以写入edits。
客户端fencing，确保只有一个NN可以响应客户端的请求,
DN fencing，确保只有一个NN向DN下发命令，譬如删除块，复制块等等。

### 角色分配
主机名 | IP | 角色
---|---|---
nna | 192.168.11.10 | active
nns | 192.168.11.12 | standby
dn1 | 192.168.11.100 | zookeeper
dn2 | 192.168.11.120 | zookeeper
dn3 | 192.168.11.130 | zookeeper

### 环境变量配置

``` sh
[root@ ~]# 
export JAVA_HOME=/opt/jdk
export PATH=${JAVA_HOME}/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/jre/lib/dt.jar:$JAVA_HOME/lib/tools.jar
[hadoop@ ~]$ vim .bashrc
export JAVA_HOME=/opt/jdk
export PATH=${JAVA_HOME}/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/jre/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$PATH:/opt/source/hadoop/bin:/opt/source/hadoop/sbin
```

### 配置文件

- core-site.xml

``` sh
[hadoop@ ~]$ cat core-site.xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://cluster1</value>
    </property>

    <property>
        <name>io.file.buffer.size</name>
        <value>131072</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/hadoop/tmp</value>
    </property>
    <property>
        <name>hadoop.proxyuser.hadoop.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.hadoop.groups</name>
        <value>*</value>
    </property>
    <property>
        <name>ha.zookeeper.quorum</name>
        <value>dn1:2181,dn2:2181,dn3:2181</value>
    </property>
</configuration>

```

- hdfs-site.xml

``` sh
[hadoop@ ~]$ cat hdfs-site.xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>dfs.nameservices</name>
        <value>cluster1</value>
    </property>
    <property>
        <name>dfs.ha.namenodes.cluster1</name>
        <value>nna,nns</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.cluster1.nna</name>
        <value>nna:9000</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.cluster1.nns</name>
        <value>nns:9000</value>
    </property>

    <property>
        <name>dfs.namenode.http-address.cluster1.nna</name>
        <value>nna:50070</value>
    </property>

    <property>
        <name>dfs.namenode.http-address.cluster1.nns</name>
        <value>nns:50070</value>
    </property>
    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://dn1:8485;dn2:8485;dn3:8485/cluster1</value>
    </property>
    <!-- 配置失败自动切换实现方式 --> 
    <property>
        <name>dfs.client.failover.proxy.provider.cluster1</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
    </property>
    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/home/hadoop/.ssh/id_rsa</value>
    </property>
    <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/home/hadoop/data/tmp/journal</value>
    </property>
    <!-- 开启NameNode失败自动切换 -->
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/home/hadoop/data/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/home/hadoop/data/dfs/data</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
    </property>

    <property>
        <name>dfs.journalnode.http-address</name>
        <value>0.0.0.0:8480</value>
    </property>
    <property>
        <name>dfs.journalnode.rpc-address</name>
        <value>0.0.0.0:8485</value>
    </property>
    <property>
        <name>ha.zookeeper.quorum</name>
        <value>dn1:2181,dn2:2181,dn3:2181</value>
    </property>

</configuration>
```

- map-site.xml

``` sh
[hadoop@ ~]$ cat map-site.xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>nna:10020</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>nna:19888</value>
    </property>
</configuration>
```

- yarn-site.xml
 
``` sh
[hadoop@ ~]$ cat yarn-site.xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>yarn.resourcemanager.connect.retry-interval.ms</name>
        <value>2000</value>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.rm-ids</name>
        <value>rm1,rm2</value>
    </property>
    <property>
        <name>ha.zookeeper.quorum</name>
        <value>dn1:2181,dn2:2181,dn3:2181</value>
    </property>

    <property>
        <name>yarn.resourcemanager.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm1</name>
        <value>nna</value>
    </property>

    <property>
        <name>yarn.resourcemanager.hostname.rm2</name>
        <value>nns</value>
    </property>
    <!--在namenode1上配置rm1,在namenode2上配置rm2,注意：一般都喜欢把配置好的文件远程复制到其它机器上，但这个在YARN的另一个机器上一定要修改 -->
    <property>
        <name>yarn.resourcemanager.ha.id</name>
        <value>rm1</value>
    </property>
    <!--开启自动恢复功能 -->
    <property>
        <name>yarn.resourcemanager.recovery.enabled</name>
        <value>true</value>
    </property>
    <!--配置与zookeeper的连接地址 -->
    <property>
        <name>yarn.resourcemanager.zk-state-store.address</name>
        <value>dn1:2181,dn2:2181,dn3:2181</value>
    </property>
    <property>
        <name>yarn.resourcemanager.store.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
    </property>
    <property>
        <name>yarn.resourcemanager.zk-address</name>
        <value>dn1:2181,dn2:2181,dn3:2181</value>
    </property>
    <property>
        <name>yarn.resourcemanager.cluster-id</name>
        <value>cluster1-yarn</value>
    </property>
    <!--schelduler失联等待连接时间 -->
    <property>
        <name>yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms</name>
        <value>5000</value>
    </property>
    <!--配置rm1 -->
    <property>
        <name>yarn.resourcemanager.address.rm1</name>
        <value>nna:8132</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address.rm1</name>
        <value>nna:8130</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address.rm1</name>
        <value>nna:8188</value>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address.rm1</name>
        <value>nna:8131</value>
    </property>
    <property>
        <name>yarn.resourcemanager.admin.address.rm1</name>
        <value>nna:8033</value>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.admin.address.rm1</name>
        <value>nna:23142</value>
    </property>
    <!--配置rm2 -->
    <property>
        <name>yarn.resourcemanager.address.rm2</name>
        <value>nns:8132</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address.rm2</name>
        <value>nns:8130</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address.rm2</name>
        <value>nns:8188</value>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address.rm2</name>
        <value>nns:8131</value>
    </property>
    <property>
        <name>yarn.resourcemanager.admin.address.rm2</name>
        <value>nns:8033</value>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.admin.address.rm2</name>
        <value>nns:23142</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
        <name>yarn.nodemanager.local-dirs</name>
        <value>/home/hadoop/data/yarn/local</value>
    </property>
    <property>
        <name>yarn.nodemanager.log-dirs</name>
        <value>/home/hadoop/log/yarn</value>
    </property>
    <property>
        <name>mapreduce.shuffle.port</name>
        <value>23080</value>
    </property>
    <!--故障处理类 -->
    <property>
        <name>yarn.client.failover-proxy-provider</name>
        <value>org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider</value>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.automatic-failover.zk-base-path</name>
        <value>/yarn-leader-election</value>
    </property>
</configuration>
```

### 2.12 slaves

``` sh
[hadoop@ ~]$ vim /opt/source/hadoop/etc/hadoop/slaves
[hadoop@ ~]$ cat slaves
dn1
dn2
dn3
```

### 启动命令（hdfs和yarn的相关命令）
#### 启动zookeeper(个节点分别执行)
```
[root@dn1 ~]# /opt/source/zookeeper/bin/zkServer.sh start
```
#### datanode 分别在各个节点执行:
``` bash
[hadoop@dn1 ~]$ hadoop-daemon.sh start journalnode
[hadoop@dn2 ~]$ hadoop-daemon.sh start journalnode
[hadoop@dn3 ~]$ hadoop-daemon.sh start journalnode

[hadoop@dn3 ~]$ jps   [各个节点均会显示 JournalNode]
2658 Jps
2627 JournalNode
2557 QuorumPeerMain
```

#### 在主节点(nna)操作如下：

``` sh
[hadoop@nna ~]$ hadoop namenode -format
[hadoop@nna ~]$ hdfs zkfc -formatZK
[hadoop@nna ~]$ start-dfs.sh
....
首次运行会提示yes/no 确认  根据对应提示输入yes即可
--- 实验中突然卡住,以为出问题...然后重复输入yes后继续运行，所以如果发现
卡住不动的情况,尝试再次输入yes 应答！
....
[hadoop@nna ~]$ start-yarn.sh 
starting yarn daemons
starting resourcemanager, logging to /opt/source/hadoop/logs/yarn-hadoop-resourcemanager-es1.out
dn3: starting nodemanager, logging to /opt/source/hadoop/logs/yarn-hadoop-nodemanager-es5.out
dn1: starting nodemanager, logging to /opt/source/hadoop/logs/yarn-hadoop-nodemanager-es3.out
dn2: starting nodemanager, logging to /opt/source/hadoop/logs/yarn-hadoop-nodemanager-es4.out
```

#### 最后在nns(stanby)节点执行：

``` sh
[hadoop@nns ~]$ hdfs namenode -bootstrapStandby # 同步NNA节点的元数据
[hadoop@nns ~]$ hadoop-daemon.sh start namenode 
[hadoop@nns ~]$ yarn-daemon.sh start resourcemanager
[hadoop@nns ~]$ jps
3408 Jps
2579 QuorumPeerMain
2868 ResourceManager
2949 NameNode
2694 DFSZKFailoverController
```
### NameNode 的 active 和 standby 状态切换
>这条命令的意思是，将nna变成standby，nns变成active。而且手动状态下需要重启服务

``` sh
[hadoop@nna ~]$ hdfs haadmin -failover --forcefence --forceactive nna nns
```

### 状态查看：
``` bash
[hadoop@nna ~]$ hdfs haadmin -getServiceState nna
active
[hadoop@nna ~]$ hdfs haadmin -getServiceState nns
standby
```
#### 未设置自动切换状态，手动切换：
namenode 状态切换到 active 或 standby 状态

```
# hdfs haadmin -transitionToActive nns 
# hdfs haadmin -transitionToStandby nna 
```

#### 截图
![image](https://dn-linuxcn.qbox.me/data/attachment/album/201502/25/160926wz5wmqnqz6s9pjsq.png)
![image](https://dn-linuxcn.qbox.me/data/attachment/album/201502/25/160926py5yyrb2k85o8ozr.png)
![image](https://dn-linuxcn.qbox.me/data/attachment/album/201502/25/160926occllsoc8ylcodo1.png)
![image](https://dn-linuxcn.qbox.me/data/attachment/album/201502/25/160926xqbmrtq3b2c55z8z.png)


转载请注明出处，本文采用 [CC4.0](http://creativecommons.org/licenses/by-nc-nd/4.0/) 协议授权
